

## Prepare the VLMs

There are two steps of adversarial attack for VLMs: (1) transfer-based attacking strategy and (2) query-based attacking strategy for the further improvement.

### Building a suitable LAVIS environment
```
conda create -n lavis python=3.8
conda activate lavis

git clone https://github.com/salesforce/LAVIS.git
cd LAVIS
pip install -e .
```
or following the steps [HERE](https://github.com/salesforce/LAVIS), and you can refer to the [ModelZoo](https://opensource.salesforce.com/LAVIS//latest/getting_started.html#model-zoo) for the possible model candidates.

## <b> Example: BLIP </b>

### Transfer-based attacking strategy

```
python _train_adv_img_blip.py  \
 --batch_size 50 \
 --num_samples 10000 \
 --steps 300 \
 --output "blip_adv" \
 --model_name "blip_caption" \
 --model_type "base_coco" \
```
the crafted adv images x_trans will be stored in `../_output_img/name_of_your_output_img_folder`. Then, we perform image-to-text and store the generated response of x_trans. This can be achieved by:

```
python _lavis_img2txt.py  \
 --batch_size 50 \
 --num_samples 10000 \
 --img_path '../_output_img/blip_1_adv' \
 --output_path "blip_1_adv" \
 --model_name "blip_caption" \
 --model_type "base_coco" \

```
where the generated responses will be stored in `./output_unidiffuser/name_of_your_output_txt_file.txt`. We will use them for pseudo-gradient estimation via RGF-estimator.

### Query-based attacking strategy (via RGF-estimator)

```
python _train_adv_img_query.py \
 --text_path '../_output_text/blip_1_adv_pred.txt' \
 --model_name blip_caption \
 --model_type base_coco \
 --batch_size 1 \
 --num_samples 1000 \
 --steps 8 \
 --sigma 8 \
 --delta 'zero' \
 --num_query 50 \
 --num_sub_query 50 \
 --wandb \
 --wandb_project_name blip_1_adv_query \
 --wandb_run_name sigma_8_zero_delta \
```



# Bibtex
If you find this project useful in your research, please consider citing our paper:

```
@article{zhao2023evaluate,
  title={On Evaluating Adversarial Robustness of Large Vision-Language Models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
  journal={arXiv preprint arXiv:2305.16934},
  year={2023}
}
```

Meanwhile, a relevant research that aims to [Embedding a Watermark to (multi-modal) Diffusion Models](https://github.com/yunqing-me/WatermarkDM):
```
@article{zhao2023recipe,
  title={A Recipe for Watermarking Diffusion Models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Cheung, Ngai-Man and Lin, Min},
  journal={arXiv preprint arXiv:2303.10137},
  year={2023}
}
```

# Acknowledgement: 

We appreciate the wonderful base implementation of [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLaVA](https://llava-vl.github.io/), [Unidiffuser](https://github.com/thu-ml/unidiffuser), [LAVIS](https://github.com/salesforce/LAVIS) and [CLIP](https://openai.com/research/clip). 
We also thank [@MetaAI](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) for open-sourcing their LLaMA checkponts. We thank SiSi for providing some enjoyable and visual-pleasant images generated by [@Midjourney](https://www.midjourney.com/app/) in our research.

